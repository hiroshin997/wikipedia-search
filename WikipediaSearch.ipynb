{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Search\n",
    "This notebook is to experiment to build search index of Wikipedia and the interfaces to retrieve similar documents to requested text by a user. Here are the traits\n",
    "* All words and phrases in documents are translated to word embedings using word2vec\n",
    "* Building binary tree index from document vectors using k-means clustering\n",
    "* Beam search to traverse the binary tree to retrieve similar documents fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import codecs\n",
    "import copy\n",
    "from collections import deque\n",
    "import datetime\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "from sklearn.cluster import KMeans\n",
    "import sys\n",
    "import time\n",
    "import unicodedata\n",
    "import urllib.request\n",
    "\n",
    "import MeCab\n",
    "import pygtrie\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO)\n",
    "\n",
    "projectDir = \"WikiSearch\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download wikipedia file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://dumps.wikimedia.org/jawiki/latest/jawiki-latest-pages-articles.xml.bz2\"\n",
    "\n",
    "compressedFile = os.path.join(projectDir, url.split(\"/\")[-1])\n",
    "\n",
    "if not os.path.exists(compressedFile):\n",
    "    start = time.time()\n",
    "    urllib.request.urlretrieve(url, compressedFile)\n",
    "    logging.info(\"completed to download {} (elapsed time:{:.3f})\".format(compressedFile, time.time() - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read pages of title and text parts one by one and save to TSV file\n",
    "\n",
    "**NOTE:** Please download WikiExtractor.py from https://github.com/attardi/wikiextractor and locate it under the same directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputDir = os.path.join(projectDir, \"dump\")\n",
    "numberOfprocesses = 4\n",
    "outputFileSize = 1000000000\n",
    "\n",
    "if not os.path.isdir(outputDir):\n",
    "    start = time.time()\n",
    "    os.makedirs(outputDir)\n",
    "    os.system(\"WikiExtractor.py -q -o {} --processes {} -b {} {}\".format(outputDir, numberOfprocesses, outputFileSize, compressedFile))\n",
    "    logging.info(\"completed extract documents by WikiExtractor.py (elapsed time:{:.3f})\".format(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract all artiles and output to one TSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractArticle(lines):\n",
    "    title = None\n",
    "    body = None\n",
    "    if len(lines) > 1:\n",
    "        title = lines[1]\n",
    "        body = \"\".join(lines[2:-1])\n",
    "        body = body.replace(\"\\n\", \" \")\n",
    "        \n",
    "    return title.strip(), body.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiTsvFile = os.path.join(projectDir, \"wikiArticle.tsv\")\n",
    "\n",
    "if not os.path.exists(wikiTsvFile):\n",
    "    start = time.time()\n",
    "    logging.info(\"start extracting title and article per line\")\n",
    "    with open(wikiTsvFile, \"w\", encoding=\"utf-8\") as wf:\n",
    "        count = 0\n",
    "        thisDir = os.getcwd()\n",
    "        for curDir, _, files in os.walk(thisDir):\n",
    "            for file in files:\n",
    "                if file.startswith(\"wiki_\"):\n",
    "                    with codecs.open(os.path.join(curDir,file), \"r\", \"utf-8\") as rf:\n",
    "                        isInPage = False\n",
    "                        line = rf.readline()\n",
    "\n",
    "                        while line:\n",
    "                            if line.startswith(\"<doc id=\"):\n",
    "                                isInPage = True\n",
    "                                lines = []\n",
    "                            elif \"</doc>\" in line:\n",
    "                                lines.append(line)\n",
    "                                title, body = extractArticle(lines)\n",
    "                                if not \"(曖昧さ回避)\" in title:\n",
    "                                    wf.write(\"{}\\t{}\\n\".format(title, body))\n",
    "\n",
    "                                    count += 1\n",
    "                                    #if count >= 1500:\n",
    "                                    #    break\n",
    "\n",
    "                                    if count % 100000 == 0:\n",
    "                                        logging.info(\"processed {:,} documents\".format(count))\n",
    "\n",
    "                                lines = []\n",
    "                                isInPage = False                            \n",
    "                                \n",
    "                            if isInPage:\n",
    "                                lines.append(line)\n",
    "\n",
    "                            line = rf.readline()\n",
    "                        \n",
    "        logging.info(\"completed to output {:,} documents to {} (elapsed time:{:.3f})\".format(count, wikiTsvFile, time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize text with MeCab\n",
    "\n",
    "MeCab is downloaded from https://pypi.org/project/mecab/ and installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = MeCab.Tagger (\"-Ochasen\")\n",
    "\n",
    "def tokenize(text):\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    array = [x.split(\"\\t\") for x in mecab.parse(text).split(\"\\n\") if x.count(\"\\t\") == 5]\n",
    "    return [[x[0] for x in array], [x[3] for x in array]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build Trie index for words and phrases\n",
    "For trie library, pygtrie is used from https://pygtrie.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrieNode:\n",
    "    def __init__(self, poses):\n",
    "        self.poses = poses\n",
    "        self.vector = None\n",
    "        self.df = 0\n",
    "\n",
    "def buildTrie(wikiTsvFile):\n",
    "    start = time.time()\n",
    "    logging.info(\"start building trie\")\n",
    "    trie = pygtrie.Trie()\n",
    "    curDir = os.getcwd()\n",
    "    with codecs.open(os.path.join(curDir,wikiTsvFile), \"r\", \"utf-8\") as f:\n",
    "        line = f.readline()\n",
    "        count = 0\n",
    "        while line:\n",
    "            title = line.split(\"\\t\")[0]\n",
    "            ret = tokenize(title)\n",
    "            if len(ret[0]) > 1:\n",
    "                key = str.join(\"_\", ret[0])\n",
    "                trie[key] = TrieNode(ret[1])\n",
    "            count += 1\n",
    "            if count % 100000 == 0:\n",
    "                logging.info(\"processed {:,} titles\".format(count))\n",
    "            line = f.readline()\n",
    "            \n",
    "    logging.info(\"completed to build trie with {:,} titles (elapsed time:{:.3f})\".format(count, time.time() - start))\n",
    "    return trie\n",
    "\n",
    "def saveTrie(trie, filename):\n",
    "    start = time.time()\n",
    "    logging.info(\"start saving trie file {}\".format(filename))\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(trie, f)\n",
    "    logging.info(\"completed to save trie to {}(elapsed time:{:.3f})\".format(filename, time.time() - start))\n",
    "    \n",
    "def loadTrie(filename):\n",
    "    start = time.time()\n",
    "    logging.info(\"start loading trie file {}\".format(filename))\n",
    "    with open(filename, 'rb') as f:\n",
    "        trie = pickle.load(f)\n",
    "    logging.info(\"completed to load trie file {} (elapsed time:{:.3f})\".format(trieWithoutVecFile, time.time() - start))\n",
    "    return trie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-02 17:03:14,956 : INFO : start loading trie file WikiSearch\\trieWithoutVec.pickle\n",
      "2020-09-02 17:03:46,238 : INFO : completed to load trie file WikiSearch\\trieWithoutVec.pickle (elapsed time:31.282)\n"
     ]
    }
   ],
   "source": [
    "trieWithoutVecFile = os.path.join(projectDir, \"trieWithoutVec.pickle\")\n",
    "\n",
    "if not os.path.exists(trieWithoutVecFile):\n",
    "    trie = buildTrie(wikiTsvFile)\n",
    "    saveTrie(trie, trieWithoutVecFile)\n",
    "else:\n",
    "    trie = loadTrie(trieWithoutVecFile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize sentenses of Wikipedia descriptions for Word2Vec training\n",
    "When tokenizing wikipedia titles and the titles consists of multiple words, the words are handled as phrases. In a text, if sequential words matching with one of a phrase are found, POSs of the words are checked to match with the wikipedia title. In case both match, the words are dealt with a phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def buildWord2VecTrainFile(trie, wikiTsvFile, sentenceFile):\n",
    "    start = time.time()\n",
    "    logging.info(\"start building Word2Vec train file\")\n",
    "    with codecs.open(wikiTsvFile, \"r\", \"utf-8\") as rf, codecs.open(sentenceFile, \"w\", \"utf-8\") as wf:\n",
    "        line = rf.readline()\n",
    "        count = 0\n",
    "        while line:\n",
    "            columns = line.split(\"\\t\")\n",
    "            \n",
    "            if len(columns) != 2:\n",
    "                line = rf.readline()\n",
    "                continue\n",
    "            \n",
    "            tokens = tokenize(columns[1])\n",
    "            sentences = []\n",
    "            index = 0\n",
    "            tokenLen = 1\n",
    "            lastMatchedTokenLen = 1\n",
    "            lastMatchedSentence = None\n",
    "            \n",
    "            while index + tokenLen <= len(tokens[0]):\n",
    "                sentence = str.join(\"_\", tokens[0][index:index + tokenLen])\n",
    "                nodeStatus = trie.has_node(sentence)\n",
    "                \n",
    "                if nodeStatus & pygtrie.Trie.HAS_VALUE != 0:\n",
    "                    # check if POSs are equal too\n",
    "                    if trie[sentence].poses == tokens[1][index:index + tokenLen]:\n",
    "                        lastMatchedTokenLen = tokenLen\n",
    "                        lastMatchedSentence = sentence\n",
    "                    tokenLen += 1\n",
    "                elif nodeStatus == pygtrie.Trie.HAS_SUBTRIE:\n",
    "                    tokenLen += 1\n",
    "                else:\n",
    "                    if lastMatchedTokenLen == 1:\n",
    "                        sentences.append(tokens[0][index])\n",
    "                    else:\n",
    "                        sentences.append(lastMatchedSentence)\n",
    "                    index += lastMatchedTokenLen\n",
    "                    tokenLen = 2\n",
    "                    lastMatchedTokenLen = 1\n",
    "            \n",
    "            if index < len(tokens[0]):\n",
    "                if lastMatchedTokenLen == 1:\n",
    "                    sentences.append(tokens[0][index])\n",
    "                else:\n",
    "                    sentences.append(str.join(\"_\", tokens[0][index:index + lastMatchedTokenLen]))\n",
    "            for i in range(index + lastMatchedTokenLen, len(tokens[0])):\n",
    "                sentences.append(tokens[i])\n",
    "                \n",
    "            wf.write(\"{}\\t{}\\n\".format(columns[0], str.join(\" \", sentences)))\n",
    "            count += 1\n",
    "            if count % 100000 == 0:\n",
    "                logging.info(\"processed {:,} titles\".format(count))\n",
    "            line = rf.readline()\n",
    "                \n",
    "    logging.info(\"completed to build Word2Vec training file {} that consists of {:,} titles (elapsed time:{:.3f})\".format(sentenceFile, count, time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenSentenceFile = os.path.join(projectDir, \"tokenSentence.tsv\")\n",
    "\n",
    "if not os.path.exists(tokenSentenceFile):\n",
    "    buildWord2VecTrainFile(trie, wikiTsvFile, tokenSentenceFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create corpus file for Word2Vec Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusFile = tokenSentenceFile.replace(\".tsv\", \".corpus\")\n",
    "\n",
    "if not os.path.exists(corpusFile):\n",
    "    start = time.time()\n",
    "    logging.info(\"start creating corpus file for Word2Vec\")\n",
    "    with codecs.open(tokenSentenceFile, \"r\", \"utf-8\") as rf, codecs.open(corpusFile, \"w\", \"utf-8\") as wf:\n",
    "        line = rf.readline()\n",
    "        count = 0\n",
    "        while line:\n",
    "            columns = line.split(\"\\t\")\n",
    "            \n",
    "            if len(columns) < 2 or not columns[1]:\n",
    "                line = rf.readline()\n",
    "                continue\n",
    "                \n",
    "            wf.write(\"{}\".format(columns[1]))\n",
    "            line = rf.readline()\n",
    "            count += 1\n",
    "\n",
    "    logging.info(\"completed to create corpus file for Word2Vec training {} that consists of {:,} articles (elapsed time:{:.3f})\".format(corpusFile, count, time.time() - start))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Training\n",
    "\n",
    "**References**\n",
    "* gensim - models.word2vec – Word2vec embeddings\n",
    "> https://radimrehurek.com/gensim/models/word2vec.html\n",
    "* Tracking loss and embeddings in Gensim word2vec model\n",
    "> https://stackoverflow.com/questions/54422810/tracking-loss-and-embeddings-in-gensim-word2vec-model/54423541#54423541"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-02 17:03:46,337 : INFO : start loading Word2Vec model from WikiSearch\\tokenSentenceModel.pickle\n",
      "2020-09-02 17:03:46,338 : INFO : loading Word2Vec object from WikiSearch\\tokenSentenceModel.pickle\n",
      "2020-09-02 17:03:54,318 : INFO : loading wv recursively from WikiSearch\\tokenSentenceModel.pickle.wv.* with mmap=None\n",
      "2020-09-02 17:03:54,320 : INFO : loading vectors from WikiSearch\\tokenSentenceModel.pickle.wv.vectors.npy with mmap=None\n",
      "2020-09-02 17:04:02,995 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-09-02 17:04:02,997 : INFO : loading vocabulary recursively from WikiSearch\\tokenSentenceModel.pickle.vocabulary.* with mmap=None\n",
      "2020-09-02 17:04:02,997 : INFO : loading trainables recursively from WikiSearch\\tokenSentenceModel.pickle.trainables.* with mmap=None\n",
      "2020-09-02 17:04:02,998 : INFO : loading syn1neg from WikiSearch\\tokenSentenceModel.pickle.trainables.syn1neg.npy with mmap=None\n",
      "2020-09-02 17:04:11,720 : INFO : setting ignored attribute cum_table to None\n",
      "2020-09-02 17:04:11,721 : INFO : loaded WikiSearch\\tokenSentenceModel.pickle\n",
      "2020-09-02 17:04:13,720 : INFO : completed to load Word2Vec model (elapsed time:27.384)\n"
     ]
    }
   ],
   "source": [
    "class callback(CallbackAny2Vec):\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.loss_to_be_subed = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        self.epoch += 1\n",
    "        loss = model.get_latest_training_loss()\n",
    "        loss_now = loss - self.loss_to_be_subed\n",
    "        losses.append(loss_now)\n",
    "        self.loss_to_be_subed = loss\n",
    "        logging.info(\"Loss after epoch {}: {}\".format(self.epoch, loss_now))\n",
    "        model.save(\"{}.{}.model\".format(modelFilePrefix, self.epoch))\n",
    "\n",
    "\n",
    "# Word2Vec parameters\n",
    "w2vSize = 400\n",
    "w2vWindow = 8\n",
    "w2vMinCount = 10\n",
    "w2vWorkers = 6\n",
    "w2vEpochs = 5\n",
    "\n",
    "losses = []\n",
    "\n",
    "modelFilePrefix = tokenSentenceFile.replace(\".tsv\", \"\") \n",
    "modelFile = \"{}Model.pickle\".format(modelFilePrefix)\n",
    "\n",
    "start = time.time()\n",
    "    \n",
    "if not os.path.exists(modelFile):\n",
    "    logging.info(\"start training Word2Vec with {}\".format(corpusFile))\n",
    "    sentences = LineSentence(corpusFile)\n",
    "    model = Word2Vec(sentences, size=w2vSize, window=w2vWindow, min_count=w2vMinCount, workers=w2vWorkers, iter=w2vEpochs, compute_loss=True, callbacks=[callback()])\n",
    "    model.save(modelFile)\n",
    "    plt.plot(losses)\n",
    "    logging.info(\"completed to train Word2Vec model (elapsed time:{:.3f})\".format(time.time() - start))\n",
    "else:\n",
    "    logging.info(\"start loading Word2Vec model from {}\".format(modelFile))\n",
    "    model = Word2Vec.load(modelFile)\n",
    "    logging.info(\"completed to load Word2Vec model (elapsed time:{:.3f})\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save Document count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveDocumentCount(file, count):\n",
    "    start = time.time()\n",
    "    logging.info(\"start saving document count to {}\".format(file))\n",
    "    with open(file, 'w') as f:\n",
    "        f.write(str(count))\n",
    "    logging.info(\"completed to save document count file (elapsed time:{:.3f})\".format(time.time() - start))\n",
    "        \n",
    "def loadDocumentCount(file):\n",
    "    start = time.time()\n",
    "    logging.info(\"start loading document count from {}\".format(file))\n",
    "    with open(file, 'r') as f:\n",
    "        logging.info(\"completed to load document count file (elapsed time:{:.3f})\".format(time.time() - start)) \n",
    "        return int(f.readline())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Trie\n",
    "extend Trie index with Word2Vec values. Each word has following attributes\n",
    "* Vector of word\n",
    "* POSes by MeCab (*in case the word consists of multiple tokens)\n",
    "* Document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-02 17:04:13,790 : INFO : start loading trie file WikiSearch\\trieW2V.pickle\n",
      "2020-09-02 17:04:33,742 : INFO : completed to load trie file WikiSearch\\trieWithoutVec.pickle (elapsed time:19.951)\n",
      "2020-09-02 17:04:33,744 : INFO : start loading document count from WikiSearch\\documentCount.txt\n",
      "2020-09-02 17:04:33,754 : INFO : completed to load document count file (elapsed time:0.010)\n"
     ]
    }
   ],
   "source": [
    "trieW2VFile = os.path.join(projectDir, \"trieW2V.pickle\")\n",
    "documentCountFile = os.path.join(projectDir, \"documentCount.txt\")\n",
    "\n",
    "if not os.path.exists(trieW2VFile):\n",
    "    start = time.time()\n",
    "    logging.info(\"start fetching vectors to trie\")\n",
    "    trieW2V = pygtrie.Trie()\n",
    "    count = 0;\n",
    "    for word in model.wv.vocab.keys():\n",
    "        vector = np.array(model.wv.word_vec(word))\n",
    "        if len(vector) == w2vSize and not np.isnan(vector).any():\n",
    "            poses = trie[word].poses if \"_\" in word and trie.has_node(word) & pygtrie.Trie.HAS_VALUE != 0 else None\n",
    "            node = TrieNode(poses)\n",
    "            node.vector = vector\n",
    "            trieW2V[word] = node\n",
    "        \n",
    "        #print(\"word:{}, poses:{}, vecvor:{}\".format(word, node.poses, node.vector[:5]))\n",
    "        #if count > 100:\n",
    "        #    break\n",
    "            \n",
    "        count += 1\n",
    "        if count % 100000 == 0:\n",
    "            logging.info(\"processed {:,} words\".format(count))\n",
    "    logging.info(\"completed to fetch vector to {:,} trie nodes(elapsed time:{:.3f})\".format(count, time.time() - start))\n",
    "    \n",
    "    start = time.time()\n",
    "    logging.info(\"start fetching DF to trie nodes from {}\".format(corpusFile))\n",
    "    with codecs.open(corpusFile, \"r\", \"utf-8\") as f:\n",
    "        line = f.readline()\n",
    "        documentCount = 0\n",
    "        while line:\n",
    "            for word in set(line.split(\" \")):\n",
    "                if trieW2V.has_node(word) & pygtrie.Trie.HAS_VALUE != 0:\n",
    "                    trieW2V[word].df += 1\n",
    "            documentCount += 1\n",
    "            if documentCount % 100000 == 0:\n",
    "                logging.info(\"processed {:,} documents\".format(documentCount))\n",
    "            line = f.readline()\n",
    "    logging.info(\"completed to fetch DFs to trie nodes with {:,} documents (elapsed time:{:.3f})\".format(documentCount, time.time() - start))\n",
    "    saveTrie(trieW2V, trieW2VFile)\n",
    "    saveDocumentCount(documentCountFile, documentCount)\n",
    "else:\n",
    "    trieW2V = loadTrie(trieW2VFile)\n",
    "    documentCount = loadDocumentCount(documentCountFile)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate Document Vecotor by BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDocumentVector(tokens, isIgnorePos = False):\n",
    "    def initLastMatch():\n",
    "        return 1, 1, None, None\n",
    "    \n",
    "    matchedTokens = []\n",
    "    missingTokens = []\n",
    "    vector = np.empty(w2vSize)\n",
    "    index = 0\n",
    "    initLastMatch()\n",
    "    tokenLen, lastMatchedTokenLen, lastMatchedSentence, lastMatchedVector = initLastMatch()\n",
    "    \n",
    "    while index + tokenLen <= len(tokens[0]):\n",
    "        sentence = str.join(\"_\", tokens[0][index:index + tokenLen])\n",
    "        nodeStatus = trieW2V.has_node(sentence)\n",
    "        if isIgnorePos:\n",
    "            if nodeStatus & pygtrie.Trie.HAS_VALUE != 0:\n",
    "                matchedTokens.append(sentence)\n",
    "                node = trieW2V[sentence]\n",
    "                vector += node.vector * np.log(float(documentCount) / node.df)\n",
    "            else:\n",
    "                missingTokens.append(tokens[0][index])\n",
    "            index += 1\n",
    "        else:\n",
    "            if nodeStatus & pygtrie.Trie.HAS_VALUE != 0:\n",
    "                if tokenLen == 1 or trieW2V[sentence].poses == tokens[1][index:index + tokenLen]:\n",
    "                    node = trieW2V[sentence]\n",
    "                    lastMatchedTokenLen = tokenLen\n",
    "                    lastMatchedSentence = sentence\n",
    "                    lastMatchedVector = node.vector * np.log(float(documentCount) / node.df)\n",
    "                tokenLen += 1\n",
    "            elif nodeStatus == pygtrie.Trie.HAS_SUBTRIE:\n",
    "                tokenLen += 1\n",
    "            else:\n",
    "                if lastMatchedSentence:\n",
    "                    matchedTokens.append(lastMatchedSentence)\n",
    "                    vector += lastMatchedVector\n",
    "                    index += lastMatchedTokenLen\n",
    "                else:\n",
    "                    missingTokens.append(tokens[0][index])\n",
    "                    index += 1\n",
    "                tokenLen, lastMatchedTokenLen, lastMatchedSentence, lastMatchedVector = initLastMatch()\n",
    "\n",
    "    # L2 regularization\n",
    "    vector /= np.linalg.norm(vector, ord=2)\n",
    "    return (vector, matchedTokens, missingTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-02 17:04:33,802 : INFO : start loading document vector files WikiSearch\\docVector.pickle\n",
      "2020-09-02 17:05:07,340 : INFO : completed to load 1,031,231 documents from files WikiSearch\\docVector.pickle(elapsed time:33.539)\n"
     ]
    }
   ],
   "source": [
    "# if a document consists of limited number of words, or lots of missing words in Word2Vec Trie index, process of the document is skipped\n",
    "minTokenCount = 30\n",
    "maxMissingTokenRatio = 0.1\n",
    "\n",
    "docVectorFile = os.path.join(projectDir, 'docVector.pickle')\n",
    "start = time.time()\n",
    "\n",
    "if not os.path.exists(docVectorFile):\n",
    "    logging.info(\"start calculating document vector\")\n",
    "    docVectorMatrix = []\n",
    "    with codecs.open(tokenSentenceFile, \"r\", \"utf-8\") as f:\n",
    "        line = f.readline()\n",
    "        count = 0\n",
    "        while line:\n",
    "            columns = line.strip().split('\\t')\n",
    "            if len(columns) == 2 and columns[0] and columns[1]:\n",
    "                tokens = [columns[1].split(\" \"), [None] * len(columns[1])]\n",
    "                vector, matchedTokens, missingTokens = getDocumentVector(tokens, True)\n",
    "                tokenCount = len(matchedTokens) + len(missingTokens)\n",
    "                if tokenCount >= minTokenCount:\n",
    "                    maxMissingTokenCount = int(math.ceil(tokenCount * maxMissingTokenRatio))\n",
    "                    if len(missingTokens) < maxMissingTokenCount and not np.isnan(vector).any():\n",
    "                        buf = [columns[0]]\n",
    "                        buf.extend(vector.tolist())\n",
    "                        docVectorMatrix.append(buf)                        \n",
    "                        count += 1\n",
    "                        \n",
    "                        if count % 100000 == 0:\n",
    "                            logging.info(\"processed {:,} documents\".format(count))\n",
    "                            \n",
    "            line = f.readline()\n",
    "        \n",
    "        docVectorDf = pd.DataFrame(docVectorMatrix)\n",
    "        logging.info(\"completed to fetch DFs to trie nodes with {:,} documents (elapsed time:{:.3f})\".format(count, time.time() - start))\n",
    "        with open(docVectorFile, 'wb') as f:\n",
    "            pickle.dump(docVectorDf, f)\n",
    "            \n",
    "else:\n",
    "    logging.info(\"start loading document vector files {}\".format(docVectorFile))\n",
    "    with open(docVectorFile, 'rb') as f:\n",
    "        docVectorDf = pickle.load(f)\n",
    "    logging.info(\"completed to load {:,} documents from files {}(elapsed time:{:.3f})\".format(len(docVectorDf), docVectorFile, time.time() - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-means clustering for documents by document vectors\n",
    "k-means clustering documents to 2 clusters, and repeat it for each cluster iteratively until number of documents is less than or equal to minLeavesCount(=100 in this case) The result is used to build binary tree. 2 centroids of a cluster become the edges, and final clusterIds are index of the leaf nodes.\n",
    "\n",
    "**NOTE:** This part is very time consuming. On my machine (Xeon E5-1620 3.5GHz, RAM 32GB, Win10 x64, using 6 cores) it tooks about 7 days for the completion. This should be rewritten using GPU in future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-02 17:05:07,368 : INFO : start loading WikiSearch\\docVectorKmeans.pickle and WikiSearch\\centroids.pickle\n",
      "2020-09-02 17:05:28,614 : INFO : completed to load WikiSearch\\docVectorKmeans.pickle and WikiSearch\\centroids.pickle(elapsed time:54.814)\n"
     ]
    }
   ],
   "source": [
    "docVectorKmeansFile = os.path.join(projectDir, 'docVectorKmeans.pickle')\n",
    "docVectorKmeansFile_ = os.path.join(projectDir, 'docVectorKmeans.pickle_')\n",
    "centroidsFile = os.path.join(projectDir, 'centroids.pickle')\n",
    "centroidsFile_ = os.path.join(projectDir, 'centroids.pickle_')\n",
    "\n",
    "if not os.path.exists(docVectorKmeansFile):\n",
    "    start = time.time()\n",
    "\n",
    "    maxIterationCount = 100\n",
    "    minLeavesCount = 100\n",
    "    nodeIdColumnIndex = w2vSize + 1\n",
    "\n",
    "    docVectorDf[nodeIdColumnIndex] = 0\n",
    "    parentNodeIdQueue = deque([0])\n",
    "    centroids = {}\n",
    "\n",
    "    kmeans = KMeans(n_clusters=2, max_iter=maxIterationCount, n_jobs=w2vWorkers, verbose=1)\n",
    "\n",
    "    while parentNodeIdQueue:\n",
    "        parentNodeId = parentNodeIdQueue.popleft()\n",
    "        startNodeId = parentNodeId * 2 + 1\n",
    "        features = docVectorDf[docVectorDf[:][nodeIdColumnIndex] == parentNodeId]\n",
    "        features = features.iloc[:, 1:w2vSize+1]\n",
    "        kmeans_model = kmeans.fit(features)\n",
    "        predict = kmeans.predict(features) + parentNodeId * 2 + 1\n",
    "        features[nodeIdColumnIndex] = predict\n",
    "\n",
    "        for i, centroid in enumerate(kmeans_model.cluster_centers_):\n",
    "            centroids[startNodeId + i] = centroid.tolist()\n",
    "\n",
    "        for tuple in features.itertuples():\n",
    "            docVectorDf.iloc[tuple[0], nodeIdColumnIndex] = tuple[-1]\n",
    "\n",
    "        for id in range(startNodeId, startNodeId + 2):\n",
    "            if len(docVectorDf[docVectorDf[:][nodeIdColumnIndex] == id]) > minLeavesCount:\n",
    "                parentNodeIdQueue.append(id)\n",
    "\n",
    "        logging.info(\"k-means clustering has been done for nodeId {} with {:,} documents\".format(parentNodeId, len(features)))\n",
    "        with open(docVectorKmeansFile_, 'wb') as f:\n",
    "            pickle.dump(docVectorDf, f)\n",
    "        with open(centroidsFile_, 'wb') as f:\n",
    "            pickle.dump(centroids, f)\n",
    "\n",
    "    os.rename(docVectorKmeansFile_, docVectorKmeansFile)\n",
    "    os.rename(centroidsFile_, centroidsFile)\n",
    "\n",
    "    logging.info(\"completed k-means clustering for {:,} documents(elapsed time:{:.3f})\".format(len(docVectorDf), time.time() - start))\n",
    "    \n",
    "else:\n",
    "    logging.info(\"start loading {} and {}\".format(docVectorKmeansFile, centroidsFile))\n",
    "    with open(docVectorKmeansFile, 'rb') as f:\n",
    "        docVectorDf = pickle.load(f)\n",
    "    with open(centroidsFile, 'rb') as f:\n",
    "        centroids = pickle.load(f)\n",
    "    logging.info(\"completed to load {} and {}(elapsed time:{:.3f})\".format(docVectorKmeansFile, centroidsFile, time.time() - start))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implement beam search for clustered documents\n",
    "steps\n",
    "1. tokenize requested text and translate to vector\n",
    "2. from top node start traversing binary tree with limited BFS, calculating cosine similarity between the text vector and centroids\n",
    "3. When completing traversing, get clusterIds of the leaf nodes and aggregate all of the associated documents\n",
    "4. Calculate cosine similarity between user requested text and wikipedia document one by one to come up with the most similar wikipedia documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, clusterId):\n",
    "        self.clusterId = clusterId\n",
    "        self.similarity = 0.\n",
    "        self.isLeaf = False\n",
    "        \n",
    "    def getLeftClusterId(self):\n",
    "        return 2 * self.clusterId + 1\n",
    "    \n",
    "    def getRightClusterId(self):\n",
    "        return 2 * (self.clusterId + 1)\n",
    "\n",
    "class SimilarDocument:\n",
    "    def __init__(self, clusterId, title, similarity):\n",
    "        self.clusterId = clusterId\n",
    "        self.title = title\n",
    "        self.similarity = similarity\n",
    "    \n",
    "class NodeQueue:\n",
    "    def __init__(self, docVectorDf, centroids, maxBreadth):\n",
    "        self.docVectorDf = docVectorDf\n",
    "        self.clusterIdSet = set(docVectorDf[:][w2vSize + 1].unique())\n",
    "        self.centroids = centroids\n",
    "        self.maxBreadth = maxBreadth\n",
    "        self.queue = []\n",
    "    \n",
    "    def getNodes(self):\n",
    "        return [(i, n) for i, n in enumerate(self.queue) if not n.isLeaf]\n",
    "\n",
    "    def getAllLeaves(self):\n",
    "        return sorted([n for n in self.queue if n.isLeaf], key=lambda n: n.similarity, reverse=True)\n",
    "    \n",
    "    def getSimilarDocuments(self, docVector):\n",
    "        similarDocuments = []\n",
    "        conditions = None\n",
    "        for leaf in self.getAllLeaves():\n",
    "            if conditions is None:\n",
    "                conditions = self.docVectorDf[:][w2vSize + 1] == leaf.clusterId\n",
    "            else:\n",
    "                conditions |= self.docVectorDf[:][w2vSize + 1] == leaf.clusterId\n",
    "        for tuple in self.docVectorDf[conditions].itertuples():\n",
    "            clusterId = tuple[0]\n",
    "            title = tuple[1]\n",
    "            similarity = np.dot(docVector, tuple[2:w2vSize + 2])\n",
    "            similarDocuments.append(SimilarDocument(clusterId, title, similarity))\n",
    "        return sorted(similarDocuments, key=lambda d: d.similarity, reverse=True)\n",
    "                                    \n",
    "    def hasNode(self):\n",
    "        return len(self.getNodes()) > 0\n",
    "    \n",
    "    def popLeft(self):\n",
    "        node = None\n",
    "        nodeTuples = self.getNodes()\n",
    "        if nodeTuples:\n",
    "            node = nodeTuples[0][1]\n",
    "            del self.queue[nodeTuples[0][0]]\n",
    "        return node\n",
    "    \n",
    "    def append(self, docVector, node):\n",
    "        node.isLeaf = node.clusterId in self.clusterIdSet\n",
    "        node.similarity = np.dot(docVector, self.centroids[node.clusterId])\n",
    "        self.queue.append(node)\n",
    "        while len(self.queue) > self.maxBreadth:\n",
    "            minSimilarity = 1.\n",
    "            minIndex = -1\n",
    "            for i, n in enumerate(self.queue):\n",
    "                if n.similarity < minSimilarity:\n",
    "                    minSimilarity = n.similarity\n",
    "                    minIndex = i\n",
    "            del self.queue[minIndex]\n",
    "            \n",
    "    def clear(self):\n",
    "        self.queue.clear()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSimilarDocuments(nodeQueue, docVector):\n",
    "    nodeQueue.clear()\n",
    "    nodeQueue.append(docVector, Node(1))\n",
    "    nodeQueue.append(docVector, Node(2))\n",
    "    while nodeQueue.hasNode():\n",
    "        node = nodeQueue.popLeft()\n",
    "        nodeQueue.append(docVector, Node(node.getLeftClusterId()))\n",
    "        nodeQueue.append(docVector, Node(node.getRightClusterId()))\n",
    "    return nodeQueue.getSimilarDocuments(docVector)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopXSimilarDocuments(nodeQueue, topX, text):\n",
    "    start = datetime.datetime.now()\n",
    "    tokens = tokenize(text)\n",
    "    vector, matchedTokens, missingTokens = getDocumentVector(tokens)\n",
    "    documents = []\n",
    "    tokenCount = len(matchedTokens) + len(missingTokens)\n",
    "    missingTokenRatio = len(missingTokens) / tokenCount\n",
    "    docs = []\n",
    "    if missingTokenRatio < maxMissingTokenRatio:\n",
    "        docs = getSimilarDocuments(nodeQueue, vector)\n",
    "        for doc in docs[:topX]:\n",
    "            documents.append({'title':doc.title, 'url':'https://ja.wikipedia.org/wiki/{}'.format(urllib.parse.quote(doc.title)), 'similarity':doc.similarity})\n",
    "    end = datetime.datetime.now()\n",
    "    response = {\n",
    "        'request':\n",
    "        {\n",
    "            'requestTime':start.strftime('%Y-%m-%dT%H:%M:%S'),\n",
    "            'text':text,\n",
    "            'tokenCount':tokenCount,\n",
    "            'missingTokenRatio':missingTokenRatio,\n",
    "            'missingTokens':missingTokens\n",
    "        },\n",
    "        'response':\n",
    "        {\n",
    "            'responseTime':end.strftime('%Y-%m-%dT%H:%M:%S'),\n",
    "            'documentCount':len(docs),\n",
    "            'documents':documents,\n",
    "        },\n",
    "        'elapsedMilliseconds':int(round((end - start).total_seconds(), 4) * 1000)\n",
    "    }\n",
    "    return json.dumps(response, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"request\": {\"requestTime\": \"2020-09-02T17:59:41\", \"text\": \"地球温暖化により台風の勢力が拡大している懸念について\", \"tokenCount\": 10, \"missingTokenRatio\": 0.0, \"missingTokens\": []}, \"response\": {\"responseTime\": \"2020-09-02T17:59:41\", \"documentCount\": 653, \"documents\": [{\"title\": \"台風\", \"url\": \"https://ja.wikipedia.org/wiki/%E5%8F%B0%E9%A2%A8\", \"similarity\": 0.6928481710987686}, {\"title\": \"2015年\", \"url\": \"https://ja.wikipedia.org/wiki/2015%E5%B9%B4\", \"similarity\": 0.6775010330686655}, {\"title\": \"複雑な動きをする台風\", \"url\": \"https://ja.wikipedia.org/wiki/%E8%A4%87%E9%9B%91%E3%81%AA%E5%8B%95%E3%81%8D%E3%82%92%E3%81%99%E3%82%8B%E5%8F%B0%E9%A2%A8\", \"similarity\": 0.6726890610325177}, {\"title\": \"斜面崩壊\", \"url\": \"https://ja.wikipedia.org/wiki/%E6%96%9C%E9%9D%A2%E5%B4%A9%E5%A3%8A\", \"similarity\": 0.6506094006276739}, {\"title\": \"ハリケーン・カタリーナ\", \"url\": \"https://ja.wikipedia.org/wiki/%E3%83%8F%E3%83%AA%E3%82%B1%E3%83%BC%E3%83%B3%E3%83%BB%E3%82%AB%E3%82%BF%E3%83%AA%E3%83%BC%E3%83%8A\", \"similarity\": 0.6463698809338628}, {\"title\": \"沖縄台風\", \"url\": \"https://ja.wikipedia.org/wiki/%E6%B2%96%E7%B8%84%E5%8F%B0%E9%A2%A8\", \"similarity\": 0.6416264767397819}, {\"title\": \"高潮\", \"url\": \"https://ja.wikipedia.org/wiki/%E9%AB%98%E6%BD%AE\", \"similarity\": 0.6400985301188823}, {\"title\": \"ダイポールモード現象\", \"url\": \"https://ja.wikipedia.org/wiki/%E3%83%80%E3%82%A4%E3%83%9D%E3%83%BC%E3%83%AB%E3%83%A2%E3%83%BC%E3%83%89%E7%8F%BE%E8%B1%A1\", \"similarity\": 0.6342137903391822}, {\"title\": \"日本列島改造論\", \"url\": \"https://ja.wikipedia.org/wiki/%E6%97%A5%E6%9C%AC%E5%88%97%E5%B3%B6%E6%94%B9%E9%80%A0%E8%AB%96\", \"similarity\": 0.6326702557849628}, {\"title\": \"オホーツク海高気圧\", \"url\": \"https://ja.wikipedia.org/wiki/%E3%82%AA%E3%83%9B%E3%83%BC%E3%83%84%E3%82%AF%E6%B5%B7%E9%AB%98%E6%B0%97%E5%9C%A7\", \"similarity\": 0.6277040941713773}]}, \"elapsedMilliseconds\": 317}'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '地球温暖化により台風の勢力が拡大している懸念について'\n",
    "\n",
    "nodeQueue = NodeQueue(docVectorDf, centroids, 10)\n",
    "\n",
    "getTopXSimilarDocuments(nodeQueue, 10, text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
